"""
#Data Analyzer Agent
This agent is Responsible for analyzing the Data recieved from the ToolNode after the Data has been fetched from the Sprinklr API. It processes and categorizes the data into buckets. Labels each bucket as a Potential Themes,Add a Name and description to That Bucket, Generate a Boolean Keyword Query Specicifc for that Bucket(That will with maximum probability get only that data that was first categorized for that bucket)and returns Such JSON.
This is the Main Agent that has the most crucial role in the first phase of the Project.

#Data Magniture : Data is an array of JSON Objects. Where the Array lenght can be in range of 2000 to 4000 Object

#Agent Knowledge Base Context:
The agent has access to some example Themes, their description, and the possible boolean keyword queries that can be used to fetch data related to those themes. This context will help the agent in categorizing the data into relevant themes and generating appropriate boolean keyword queries.


#Theme Definition: 
Theme is nothing but a filter that helps in categorizing similar Data Based on the User Input and the data Received. 
For Example, if the Refined User Prompt is "Show me samsungs Brand Mention in the Last 30 Days on Chanells like Twitter, Facebook and Instagram", then the Data Analyzer Agent will categorize the data into different themes such as "Samsung Brand Mentions", "Twitter Mentions", "Facebook Mentions","Mentions in Last 30 Days" and "Instagram Mentions". Each theme will have a name, description, and a boolean keyword query that can be used to fetch data related to that specific theme. These Themes will eventually Help the Customers to Better Filter their Data and Fetch in a More Effecient Way, therefore Reducing the Processing and Searching Cost and Improving User Experience.

#Functionality:
- Based on the data received from the ToolNode, the Data Analyzer Agent will analyze the Data and categorize it into different buckets.
- This Categorization is Done Using the final Refined user Prompt (Existing in the AgentState)  LLM or Machine Learning Models or any other method that the Agent Deems fit. Some Possible Methods can be: Zero-Shot Classification (Bart LLM), Clustering, or any other method that can be used to categorize the data into different themes.
- Each bucket will be labeled as a potential theme, and the agent will generate a name, description and a most optimal Boolean Keyword Query for each theme based on the data in that bucket.
- The agent will then return a JSON object containing the categorized themes, their names, descriptions, and the boolean keyword queries.
- The Boolean Keyword Query can be Generated using the Boolean Keyword Query Generator Agent or can be generated by the Data Analyzer Agent itself based on the Architecture and the Modularity of the Project.

#Example:
Input: 
{
    "data": [
        {
            "id": 1,
            "content": "Samsung Galaxy S21 is the best smartphone of 2021",
            "channel": "Twitter",
            "date": "2021-01-01"
        },
        {
            "id": 2,
            "content": "Samsung's new TV is amazing",
            "channel": "Facebook",
            "date": "2021-02-01"
        },
        {
            "id": 3,
            "content": "Samsung Galaxy S21 review",
            "channel": "Instagram",
            "date": "2021-03-01"
        }
        ... # In total 2000 to 4000 objects
    ]
}


Output:
The Exact Themes and their boolean keyword queries will depend on the data received and the refined user prompt. The Themes can mean something entirely different, and will be known as the System Prompts for the Data Analyzer agent will be written in the Future
{
    "themes": [
        {
            "name": "Samsung Brand Mentions",
            "description": "Data related to mentions of Samsung brand across various channels.",
            "boolean_keyword_query": "(Samsung AND (brand OR mentions))"
        },
        {
            "name": "Twitter Mentions",
            "description": "Data related to mentions of Samsung brand on Twitter.",
            "boolean_keyword_query": "(Samsung AND Twitter)"
        },
        {
            "name": "Facebook Mentions",
            "description": "Data related to mentions of Samsung brand on Facebook.",
            "boolean_keyword_query": "(Samsung AND Facebook)"
        },
        {
            "name": "Instagram Mentions",
            "description": "Data related to mentions of Samsung brand on Instagram.",
            "boolean_keyword_query": "(Samsung AND Instagram)"
        },
        {
            "name": "Mentions in Last 30 Days",
            "description": "Data related to mentions of Samsung brand in the last 30 days.",
            "boolean_keyword_query": "(Samsung AND (mentions OR reviews) AND (last 30 days))"
        }
    ]
}

"""

import logging
import json
import importlib.util
import os
from typing import Dict, Any, List, Optional
from datetime import datetime
import sys

logger = logging.getLogger(__name__)

sys.path.append(os.path.join(os.path.dirname(os.path.abspath(__file__)), 'src'))
from src.utils.files_helper import import_module_from_file
from src.agents.base.agent_base import LLMAgent, create_agent_factory

class DataAnalyzerAgent(LLMAgent):
    """
    Data Analyzer Agent for processing and categorizing fetched data into themes.
    
    This agent analyzes the data received from the API and categorizes it into
    meaningful themes with descriptions and boolean queries for each theme.
    """
    
    def __init__(self, llm=None):
        """
        Initialize the Data Analyzer Agent.
        
        Args:
            llm: Language model instance (optional, can use classification models instead)
        """
        super().__init__("data_analyzer", llm)
        
        # Initialize theme categories for classification
        self.theme_categories = {
            "brand_mentions": {
                "description": "Posts that mention the brand name directly",
                "keywords": ["brand", "company", "product", "service"]
            },
            "sentiment_analysis": {
                "description": "Posts expressing opinions or emotions about the brand",
                "keywords": ["good", "bad", "excellent", "terrible", "love", "hate"]
            },
            "product_features": {
                "description": "Discussions about specific product features or capabilities",
                "keywords": ["feature", "functionality", "specs", "performance"]
            },
            "customer_service": {
                "description": "Posts related to customer support or service experiences",
                "keywords": ["support", "help", "service", "customer", "issue"]
            },
            "competitor_comparison": {
                "description": "Posts comparing the brand with competitors",
                "keywords": ["vs", "versus", "compared", "better", "worse"]
            }
        }
    
    async def invoke(self, state) -> Any:
        """
        Main entry point for the agent - delegates to process_state.
        
        Args:
            state: The dashboard state to process
            
        Returns:
            Updated state after data analysis processing
        """
        return await self.process_state(state)
    
    def analyze_data(self, data: List[Dict[str, Any]], refined_query: str = "") -> Dict[str, Any]:
        """
        Analyze and categorize data into themes.
        
        Args:
            data: List of data objects to analyze
            refined_query: The refined user query for context
            
        Returns:
            Dictionary containing categorized themes with metadata
        """
        try:
            logger.info(f"Analyzing {len(data)} data points")
            
            if not data:
                logger.warning("No data provided for analysis")
                return {
                    "themes": [],
                    "analysis_summary": "No data available for analysis",
                    "total_data_points": 0
                }
            
            # Categorize data into themes
            themes = self._categorize_data_into_themes(data, refined_query)
            
            # Generate theme metadata
            for theme in themes:
                theme["boolean_query"] = self._generate_theme_boolean_query(theme, refined_query)
                theme["relevance_score"] = self._calculate_relevance_score(theme, refined_query)
            
            # Sort themes by relevance
            themes.sort(key=lambda x: x.get("relevance_score", 0), reverse=True)
            
            # Create analysis summary
            analysis_summary = f"Analyzed {len(data)} data points and identified {len(themes)} themes"
            
            result = {
                "themes": themes[:10],  # Return top 10 themes
                "analysis_summary": analysis_summary,
                "total_data_points": len(data),
                "timestamp": datetime.now().isoformat()
            }
            
            logger.info(f"Analysis completed successfully with {len(themes)} themes identified")
            return result
            
        except Exception as e:
            logger.error(f"Error analyzing data: {str(e)}")
            return {
                "themes": [],
                "analysis_summary": f"Analysis failed: {str(e)}",
                "total_data_points": len(data) if data else 0,
                "error": str(e)
            }
    
    def _categorize_data_into_themes(self, data: List[Dict[str, Any]], refined_query: str) -> List[Dict[str, Any]]:
        """Categorize data into themes using keyword-based classification."""
        try:
            themes = []
            
            # Group data by content similarity
            content_groups = self._group_by_content_similarity(data)
            
            for group_name, group_data in content_groups.items():
                if len(group_data) >= 2:  # Only create themes with at least 2 data points
                    theme = {
                        "name": group_name,
                        "description": f"Posts related to {group_name.lower().replace('_', ' ')}",
                        "data_count": len(group_data),
                        "sample_data": group_data[:3],  # Include sample data
                        "keywords": self._extract_keywords_from_group(group_data)
                    }
                    themes.append(theme)
            
            # If no natural groupings found, create basic themes
            if not themes and data:
                themes = self._create_basic_themes(data)
            
            return themes
            
        except Exception as e:
            logger.error(f"Error categorizing data: {str(e)}")
            return self._create_fallback_themes(data)
    
    def _group_by_content_similarity(self, data: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
        """Group data by content similarity using keyword matching."""
        groups = {}
        
        for item in data:
            content = str(item.get("content", "")).lower()
            
            # Find best matching theme category
            best_category = None
            best_score = 0
            
            for category, config in self.theme_categories.items():
                score = sum(1 for keyword in config["keywords"] if keyword in content)
                if score > best_score:
                    best_score = score
                    best_category = category
            
            # Use best category or create "general" category
            group_name = best_category if best_category else "general_mentions"
            
            if group_name not in groups:
                groups[group_name] = []
            groups[group_name].append(item)
        
        return groups
    
    def _extract_keywords_from_group(self, group_data: List[Dict[str, Any]]) -> List[str]:
        """Extract common keywords from a group of data."""
        word_counts = {}
        
        for item in group_data:
            content = str(item.get("content", "")).lower()
            words = content.split()
            
            for word in words:
                if len(word) > 3:  # Only consider words longer than 3 characters
                    word_counts[word] = word_counts.get(word, 0) + 1
        
        # Return top keywords
        sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)
        return [word for word, count in sorted_words[:5]]
    
    def _generate_theme_boolean_query(self, theme: Dict[str, Any], refined_query: str) -> str:
        """Generate a boolean query for a specific theme."""
        try:
            keywords = theme.get("keywords", [])
            theme_name = theme.get("name", "")
            
            # Build boolean query from keywords
            if keywords:
                keyword_query = " OR ".join([f'"{keyword}"' for keyword in keywords[:3]])
                return f"({keyword_query})"
            else:
                # Fallback to theme name
                return f'"{theme_name.replace("_", " ")}"'
                
        except Exception as e:
            logger.error(f"Error generating boolean query for theme: {str(e)}")
            return f'"{theme.get("name", "general")}"'
    
    def _calculate_relevance_score(self, theme: Dict[str, Any], refined_query: str) -> float:
        """Calculate relevance score for a theme."""
        try:
            base_score = theme.get("data_count", 0) * 0.1  # Base score from data count
            
            # Bonus for themes with more keywords
            keyword_bonus = len(theme.get("keywords", [])) * 0.05
            
            # Bonus if theme name appears in refined query
            if refined_query and theme.get("name", "").lower() in refined_query.lower():
                query_bonus = 0.3
            else:
                query_bonus = 0
            
            return min(base_score + keyword_bonus + query_bonus, 1.0)
            
        except Exception as e:
            logger.error(f"Error calculating relevance score: {str(e)}")
            return 0.5
    
    def _create_basic_themes(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Create basic themes when no clear categorization is found."""
        themes = [
            {
                "name": "Brand Mentions",
                "description": "General mentions of the brand or products",
                "data_count": len(data),
                "keywords": ["brand", "product", "service"],
                "sample_data": data[:3]
            }
        ]
        
        # Add channel-based themes if channel data is available
        channels = set()
        for item in data:
            if "channel" in item:
                channels.add(item["channel"])
        
        for channel in channels:
            channel_data = [item for item in data if item.get("channel") == channel]
            if len(channel_data) >= 2:
                themes.append({
                    "name": f"{channel} Mentions",
                    "description": f"Mentions from {channel}",
                    "data_count": len(channel_data),
                    "keywords": [channel.lower()],
                    "sample_data": channel_data[:3]
                })
        
        return themes
    
    def _create_fallback_themes(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Create fallback themes when analysis fails."""
        return [
            {
                "name": "General Content",
                "description": "All available content",
                "data_count": len(data),
                "keywords": ["content", "post", "mention"],
                "sample_data": data[:3] if data else [],
                "relevance_score": 0.5
            }
        ]

    async def process_state(self, state) -> Any:
        """
        Process the dashboard state for data analysis.
        
        This method analyzes the fetched data and categorizes it into themes.
        """
        try:
            logger.info("Processing state for data analysis")
            
            # Get fetched data from state
            fetched_data = getattr(state, 'fetched_data', [])
            if not fetched_data:
                logger.warning("No fetched data available for analysis")
                state.errors.append("No data available for analysis")
                state.workflow_status = "data_analysis_failed"
                return state
            
            # Get refined query for context
            refined_query = ""
            if hasattr(state, 'query_refinement_data') and state.query_refinement_data:
                if isinstance(state.query_refinement_data, dict):
                    refined_query = state.query_refinement_data.get('refined_query', '')
                else:
                    refined_query = getattr(state.query_refinement_data, 'refined_query', '')
            
            # Analyze the data
            analysis_result = self.analyze_data(fetched_data, refined_query)
            
            # Update state with analysis results
            if hasattr(state, 'theme_data'):
                state.theme_data = analysis_result
            
            if hasattr(state, 'identified_themes'):
                # Convert themes to ThemeData objects if needed
                themes = analysis_result.get('themes', [])
                state.identified_themes = themes
            
            # Update workflow status
            if hasattr(state, 'workflow_status'):
                if analysis_result.get('themes'):
                    state.workflow_status = "data_analyzed"
                else:
                    state.workflow_status = "data_analysis_completed_no_themes"
            
            if hasattr(state, 'current_stage'):
                state.current_stage = "analyzing"
            
            logger.info(f"Data analysis completed with {len(analysis_result.get('themes', []))} themes")
            return state
            
        except Exception as e:
            logger.error(f"Error processing state: {str(e)}")
            # Add error to state if it has errors attribute
            if hasattr(state, 'errors'):
                state.errors.append(f"Data analysis error: {str(e)}")
            if hasattr(state, 'workflow_status'):
                state.workflow_status = "data_analysis_failed"
            return state

# Factory function for creating DataAnalyzerAgent instances
create_data_analyzer_agent = create_agent_factory(DataAnalyzerAgent, "data_analyzer")

